{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Association mining\n",
    "\n",
    "## Objective of this assignment\n",
    "The overall objective is to understand how frequent itemsets can be extracted by\n",
    "the Apriori algorithm and be able to calculate and interpret association rules in terms of support and confidence.\n",
    "\n",
    "## ** Important: ** When handing in your homework:\n",
    "+ Hand in the notebook (and nothing else) named as follows: StudentName1_snumber_StudentName2_snumber.ipynb\n",
    "+ Provide clear and complete answers to the questions below under a separate header (not hidden somewhere in your source code), and make sure to explain your answers / motivate your choices. Add Markdown cells where necessary.\n",
    "+ Source code, output graphs, derivations, etc., should be included in the notebook.\n",
    "+ Hand-in: upload to Blackboard.\n",
    "+ Include name, student number, assignment (especially in filenames)!\n",
    "+ When working in pairs only one of you should upload the assignment, and report the name of your partner in your filename.\n",
    "+ For problems or questions: use the BB discussion board or email the student assistants.\n",
    "\n",
    "\n",
    "## Advised Reading and Exercise Material\n",
    "**The following reading material is recommended:**\n",
    "\n",
    "- Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, *Introduction to Data Mining*, section 6.\n",
    "\n",
    "\n",
    "## Additional Tools\n",
    "For this exercise you will need to load the provided *apriorimining.py* script. \n",
    "\n",
    "\n",
    "##  5.1 Association mining for course data \n",
    "We will use the Apriori algorithm to automatically mine for associations. The Apriori algorithm is an adapted version of the script found here: https://github.com/nalinaksh/Association-Rule-Mining-Python\n",
    "\n",
    "Check out the script and doc and check if you understand how the association rules are computed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Toolbox.apriorimining as apriorimining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5.1.1\n",
    "\n",
    "(0 points) Look at the data file `Data/courses.txt` into Python. The data is represented in Table 1. Inspect the file Data/courses.txt and make sure you understand how the data in Table 1 is stored in the text file.\n",
    "\n",
    "##### Table 1\n",
    "|#  |   History |Math| Biology| Spanish | Economics| Physics | Chemistry | English  |  \n",
    "| :-------------: |:-------------:| :-----------:| :----------:| :------------:|:-------------:| :------------:|  :-------------: | :-------------: |\n",
    "|student 1 | 0| 1 | 0 | 0 | 1| 1 |1 |1   \n",
    "|student 2 | 1| 1 | 1 | 0 | 0| 1 |1 |1   \n",
    "|student 3 | 0| 1 | 0 | 1 | 0| 1 |0 |1   \n",
    "|student 4 | 0| 0 | 1 | 0 | 0| 1 |1 |0   \n",
    "|student 5 | 0| 1 | 0 | 0 | 0| 1 |1 |0        \n",
    "|student 6 | 0| 1 | 1 | 0 | 0| 1 |1 |1   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2\n",
    "(1 point) We will analyze the data in Table 1 automatically using the function `apriorimining.generate_association_rules()` from the script. Analyze the data with $ minsupport  \\geq 80 \\% $ and $ minconfidence \\geq 100 \\%$.What\n",
    "are the generated association rules? What kind of conclusions can you make based on these association rules about the subjects that students took?  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter support value in %: 80\n",
      "Please enter confidence value in %: 100\n",
      "Enter the max number of rules you want to see (enter 0 to see all rules): 0\n",
      "Please enter filepath\\filename and extension: Data/courses.txt\n",
      "---------------TOP 10 FREQUENT 1-ITEMSET-------------------------\n",
      "set= { 6 },  sup= 100.0\n",
      "set= { 2 },  sup= 83.33\n",
      "set= { 7 },  sup= 83.33\n",
      "-----------------------------------------------------------------\n",
      "-------TOP 10 (or less) FREQUENT 2-ITEMSET------------------------\n",
      "set= { 2, 6 },  sup= 83.33\n",
      "set= { 6, 7 },  sup= 83.33\n",
      "------------------------------------------------------------------\n",
      "---------------------ASSOCIATION RULES------------------\n",
      "--------------------------------------------------------\n",
      "Rule #1: {  } ==> { 6 }, sup= 100.00, conf= 100.00\n",
      "\n",
      "Rule #2: { 2 } ==> { 6 }, sup= 83.33, conf= 100.00\n",
      "\n",
      "Rule #3: { 7 } ==> { 6 }, sup= 83.33, conf= 100.00\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "apriorimining.generate_association_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for 5.1.2:\n",
    "There are three rules generated: { } &rarr; {6}, {2} &rarr; {6} and {7} &rarr; {6}.  \n",
    "We can conclude that every student took Physics, from rule 1. Rule 2 says that if a students takes Math, then the student also takes Physics, and rule 3 says that if a student takes Chemistry, the student also takes Physics.  \n",
    "Rules 2 and 3 don't contain any additional information, since rule 1 encaptures rule 2 and 3 as well. This is because {2} and {7} are supersets of { }, and the output of all three rules is equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ##  5.2 Association mining for MovieLens data \n",
    "  \n",
    "  \n",
    "  In this part of the exercise we consider a Market Basket data set containing 943 users purchases of 1682 movies. A total of 100,000 movies\n",
    "have been purchased.The data set is called MovieLens100K and is provided by http://www.grouplens.org/node/73, see also the readme `MovieLensData.txt` in the data folder. The data currently considered is not the original data but modified for the apriori algorithm.\n",
    "\n",
    "#### 5.2.1\n",
    "  (0 points) The MovieLens data is stored in the file MovieLensData.txt. Inspect the file to see how the data is stored.\n",
    "\n",
    "\n",
    "#### 5.2.2 \n",
    "  (1 point) Find association rules using the function below with $ minsupport  \\geq 30 \\% $ and $ minconfidence \\geq 80 \\%$. What are the associations with strongest confidence? Do these associations make sense? The script can use file Data/u.item to print the movie titles in stead of numbers. If you enter filename `MovieLensData.txt`, the script will provide an additional option for this. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter support value in %: 30\n",
      "Please enter confidence value in %: 80\n",
      "Enter the max number of rules you want to see (enter 0 to see all rules): 10\n",
      "Please enter filepath\\filename and extension: Data/MovieLensData.txt\n",
      "Do you want to print sets and rules with Movie names in stead of numbers? [y/n]: i guess that would be nice\n",
      "---------------TOP 10 FREQUENT 1-ITEMSET-------------------------\n",
      "set= { Star Wars (1977) },  sup= 61.82\n",
      "set= { Contact (1997) },  sup= 53.98\n",
      "set= { Fargo (1996) },  sup= 53.87\n",
      "set= { Return of the Jedi (1983) },  sup= 53.76\n",
      "set= { Liar Liar (1997) },  sup= 51.43\n",
      "set= { English Patient, The (1996) },  sup= 51.01\n",
      "set= { Scream (1996) },  sup= 50.69\n",
      "set= { Toy Story (1995) },  sup= 47.93\n",
      "set= { Air Force One (1997) },  sup= 45.71\n",
      "set= { Independence Day (ID4) (1996) },  sup= 45.49\n",
      "-----------------------------------------------------------------\n",
      "-------TOP 10 (or less) FREQUENT 2-ITEMSET------------------------\n",
      "set= { Return of the Jedi (1983), Star Wars (1977) },  sup= 50.9\n",
      "set= { Fargo (1996), Star Wars (1977) },  sup= 41.78\n",
      "set= { Star Wars (1977), Toy Story (1995) },  sup= 40.4\n",
      "set= { Raiders of the Lost Ark (1981), Star Wars (1977) },  sup= 40.3\n",
      "set= { Independence Day (ID4) (1996), Star Wars (1977) },  sup= 38.39\n",
      "set= { Godfather, The (1972), Star Wars (1977) },  sup= 37.86\n",
      "set= { Fargo (1996), Return of the Jedi (1983) },  sup= 36.69\n",
      "set= { Empire Strikes Back, The (1980), Star Wars (1977) },  sup= 36.59\n",
      "set= { Raiders of the Lost Ark (1981), Return of the Jedi (1983) },  sup= 36.27\n",
      "set= { Return of the Jedi (1983), Toy Story (1995) },  sup= 36.06\n",
      "------------------------------------------------------------------\n",
      "-------TOP 10 (or less) FREQUENT 3-ITEMSET------------------------\n",
      "set= { Raiders of the Lost Ark (1981), Return of the Jedi (1983), Star Wars (1977) },  sup= 35.74\n",
      "set= { Return of the Jedi (1983), Star Wars (1977), Toy Story (1995) },  sup= 35.31\n",
      "set= { Fargo (1996), Return of the Jedi (1983), Star Wars (1977) },  sup= 34.99\n",
      "set= { Independence Day (ID4) (1996), Return of the Jedi (1983), Star Wars (1977) },  sup= 34.68\n",
      "set= { Empire Strikes Back, The (1980), Raiders of the Lost Ark (1981), Star Wars (1977) },  sup= 33.51\n",
      "set= { Empire Strikes Back, The (1980), Return of the Jedi (1983), Star Wars (1977) },  sup= 33.4\n",
      "set= { Godfather, The (1972), Return of the Jedi (1983), Star Wars (1977) },  sup= 31.92\n",
      "set= { Empire Strikes Back, The (1980), Raiders of the Lost Ark (1981), Return of the Jedi (1983) },  sup= 31.28\n",
      "set= { Raiders of the Lost Ark (1981), Silence of the Lambs, The (1991), Star Wars (1977) },  sup= 30.86\n",
      "set= { Pulp Fiction (1994), Raiders of the Lost Ark (1981), Star Wars (1977) },  sup= 30.65\n",
      "------------------------------------------------------------------\n",
      "-------TOP 10 (or less) FREQUENT 4-ITEMSET------------------------\n",
      "set= { Empire Strikes Back, The (1980), Raiders of the Lost Ark (1981), Return of the Jedi (1983), Star Wars (1977) },  sup= 31.18\n",
      "------------------------------------------------------------------\n",
      "---------------------ASSOCIATION RULES------------------\n",
      "--------------------------------------------------------\n",
      "Rule #75: { Empire Strikes Back, The (1980), Raiders of the Lost Ark (1981), Return of the Jedi (1983) } ==> { Star Wars (1977) }, sup= 31.18, conf= 99.66\n",
      "\n",
      "Rule #51: { Empire Strikes Back, The (1980), Return of the Jedi (1983) } ==> { Star Wars (1977) }, sup= 33.40, conf= 99.37\n",
      "\n",
      "Rule #32: { Pulp Fiction (1994), Return of the Jedi (1983) } ==> { Star Wars (1977) }, sup= 30.22, conf= 98.62\n",
      "\n",
      "Rule #55: { Raiders of the Lost Ark (1981), Return of the Jedi (1983) } ==> { Star Wars (1977) }, sup= 35.74, conf= 98.54\n",
      "\n",
      "Rule #57: { Return of the Jedi (1983), Toy Story (1995) } ==> { Star Wars (1977) }, sup= 35.31, conf= 97.94\n",
      "\n",
      "Rule #62: { Return of the Jedi (1983), Silence of the Lambs, The (1991) } ==> { Star Wars (1977) }, sup= 30.65, conf= 97.64\n",
      "\n",
      "Rule #59: { Return of the Jedi (1983), Twelve Monkeys (1995) } ==> { Star Wars (1977) }, sup= 30.54, conf= 97.63\n",
      "\n",
      "Rule #39: { Godfather, The (1972), Return of the Jedi (1983) } ==> { Star Wars (1977) }, sup= 31.92, conf= 97.10\n",
      "\n",
      "Rule #63: { Return of the Jedi (1983), Star Trek: First Contact (1996) } ==> { Star Wars (1977) }, sup= 30.65, conf= 96.98\n",
      "\n",
      "Rule #44: { Empire Strikes Back, The (1980), Raiders of the Lost Ark (1981) } ==> { Star Wars (1977) }, sup= 33.51, conf= 96.64\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "apriorimining.generate_association_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for 5.2.2:\n",
    "The 5 rules with the strongest confidence are:\n",
    "1. { Empire Strikes Back, The (1980), Raiders of the Lost Ark (1981), Return of the Jedi (1983) } &rarr; { Star Wars (1977) } (conf= 99.66)\n",
    "2. { Empire Strikes Back, The (1980), Return of the Jedi (1983) } &rarr; { Star Wars (1977) } (conf= 99.37)\n",
    "3. { Pulp Fiction (1994), Return of the Jedi (1983) } &rarr; { Star Wars (1977) } (conf= 98.62)\n",
    "4. { Raiders of the Lost Ark (1981), Return of the Jedi (1983) } &rarr; { Star Wars (1977) } (conf= 98.54)\n",
    "5. { Return of the Jedi (1983), Toy Story (1995) } &rarr; { Star Wars (1977) } (conf= 97.94)\n",
    "  \n",
    "Almost all of these rules make sense: most of the movies are large franchise adventure/sci-fi movies, which target the same audience. This makes it more likely that these movies will be purchased by the same users. Especially rule 2 is logical, since these are movies from the same franchise. Rule 2 and 4 are subsets of rule 1, so the explanation is similar. Rule 3 and 5 are a bit odd, since the movie genres are not very similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 \n",
    "(1 point) Which movies have been watched by the most users? There are only few rules with more than three items. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for 5.2.3:\n",
    "The movies purchased by the most users are the ones seen in the top 10 frequent 1-itemset. The top 5 is:\n",
    "1. Star Wars (1977)\n",
    "2. Contact (1997)\n",
    "3. Fargo (1996)\n",
    "4. Return of the Jedi (1983)\n",
    "5. Liar Liar (1997)\n",
    "  \n",
    "These movies have all been purchased by more than 50% of the users.  \n",
    "The reason why there are so few rules with more than three items is because there is only one frequent 4-itemset with support $\\geq$ 30%. This means that only rules consisting of a combination of these four movies could be shown. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4\n",
    "(0.5 points) Often we are interested in rules with high confidence. Is it possible for\n",
    "itemsets to have very low support but still have a very high confidence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for 5.2.4:\n",
    "A rule having low support but high confidence means that the set of all items used in the rule occurs very few times in the dataset, but when it does, the rule holds very often.  \n",
    "An example would be a series of movies, that is not popular and therefore not watched by a lot of users, but any user that watches one of them, would likely watch the others as well.  \n",
    "Another example would be to have an unpopular movie in the antecedent and a very popular movie in the consequent. It is likely that someone who watches niche movies would be a movie lover and also watch popular movies (think of very niche sci-fi and Star Wars). This means that whenever someone watches the unpopular movie, they almost always watch the popular movie, giving a high confidence, but very few people actually watch the unpopular movie, giving a low support. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Calculating support, confidence and interest\n",
    "\n",
    "Calculate these measures and write down how you computed things, not just the answers. \n",
    "\n",
    "\n",
    "#### 5.3.1\n",
    " Suppose we have market basket data consisting of 100 transactions and 20 items. The support for item $ \\text{a} = 45 \\%$, the support for item $ \\text{b} = 80 \\%$ and the support for itemset $ \\text{ {a,b }} = 30 \\%$. Let the support and confidence thresholds be 20$ \\%$ and 60$ \\%$, respectively.\n",
    "  \n",
    "1. (0.5 points) Compute the confidence of the association rule $ \\text{ {a } } \\rightarrow   \\text{{b }} $. Is the rule interesting according to the confidence measure?\n",
    "\n",
    "2. (0.5 points) Compute the interest measure (or lift, see slide 44 of chapter 6) for the association pattern $ \\text{ {a,b}}$. Describe the nature of the relationship between item $ \\text{a}$ and item $ \\text{b}$  in terms of the itemset measure.\n",
    "3. (1 points) What conclusion can you draw from the results of parts (1) and (2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for 5.3.1:\n",
    "1. $\\sigma(a,b)$ = 30, $\\sigma(a)$ = 45  \n",
    "c({a} &rarr; {b}) = $\\frac{\\sigma(a,b)}{\\sigma(a)}$ = $\\frac{30}{45}$ = $0.\\overline{6}$  \n",
    "The rule is interesting according to the confidence measure, since it is higher than the confidence threshold of 60%.  \n",
    "  \n",
    "2. Interest(a,b) = $\\frac{P(a,b)}{P(a)*P(b)}$  \n",
    "P(a,b) = 0.3, P(a) = 0.45, P(b) = 0.8  \n",
    "Interest(a,b) = $\\frac{0.3}{0.45*0.8}$ = $0.8\\overline{3}$  \n",
    "Because the interest is smaller than 1, we can conclude that items $\\text{a}$ and $\\text{b}$ are negatively correlated. This means that when item $\\text{b}$ occurs in a transaction, item $\\text{a}$ is less likely to occur than when there is no information about item $\\text{b}$ and vice versa (which can be confirmed by calculating the conditional probabilities).\n",
    "  \n",
    "3. Part (1) and (2) give conflicting information: (1) states that when $\\text{a}$ is found, $\\text{b}$ is likely to be found, while (2) states that there is a negative correlation between $\\text{a}$ and $\\text{b}$. This is likely because of the reasons stated in 5.4.2: $\\text{b}$, the consequent, has a high support of 80%. This leads to a high confidence. However, the interest tells us that there is a negative correlation. From this we can conclude that the confidence alone is not a good enough measure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2\n",
    "\n",
    "1. (1 points) Let $c_1$, $c_2$, and $c_3$ be the confidence values of the rules $ \\text{ {a } } \\rightarrow   \\text{{b }} $, $ \\text{ {a } } \\rightarrow   \\text{{b,c }} $, and $ \\text{ {a,c } } \\rightarrow   \\text{{b }} $ respectively. If we assume that $c_1$, $c_2$, and $c_3$ have different values, what are the possible inequality relationships (e.g. $c_1 \\leq c_2 \\leq c_3$) among $c_1$, $c_2$, and $c_3$? Which rule has the lowest confidence?\n",
    "2. (0.5 points) Suppose the confidence of the rules  $ \\text{ {a } } \\rightarrow   \\text{{b }} $ and $ \\text{ {b } } \\rightarrow   \\text{{c }} $ are larger than the confidence threshold. Is it possible that $ \\text{ {a } } \\rightarrow   \\text{{c }} $ has a confidence below that threshold? If no, explain why. If yes, give an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for 5.3.2:\n",
    "\n",
    "1. From the slides (Ch.6, slide 36: Rule Generation) it is clear that $c_3 \\geq c_2$, because they come from the same itemset and rule 3 has fewer items in the consequent than rule 2. Between rule 1 and 2, we can see the relationship by writing out the formula for confidence.  \n",
    "$c_1 = \\frac{\\sigma(a,b)}{\\sigma(a)}$, $c_2 = \\frac{\\sigma(a,b,c)}{\\sigma(a)}$  \n",
    "The denominators are equal, but the numerator of 2 is always smaller or equal to the numerator of 1, due to the monotone property of support. This means that $c_1 \\geq c_2$.  \n",
    "Because we know all values are different, the possible inequality relationships therefore are $c_1 > c_3 > c_2$ or $c_3 > c_1 > c_2$. This means rule 2 always has the lowest confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5.3.3\n",
    "\n",
    "(3 points) Consider the relationships between customers who buy high-definition televisions and exercise machines as shown in Table 2 and 3.\n",
    "\n",
    "1. Compute the odd ratios for both tables.\n",
    "2. Compute the $\\phi$-coefficient for both tables.\n",
    "3. Compute the interest (or lift, in the book) factor for both tables.\n",
    "\n",
    "For Table 3 you should compute measures given above separately for College\n",
    "Students and for Adults. For each of the measures, describe how the direction\n",
    "of association changes when data is pooled together (Table 2) instead of being\n",
    "separated into two groups (Table 3)\n",
    "\n",
    "##### Table 2: Two way contingency table between the sale of high-definition television and exercise machine\n",
    "| |   Buy Exercise machine |     |     |\n",
    "| :------------- | -------------:| :-----------:| :----------:| \n",
    "| **Buy HDTV     ** | yes | no | total |\n",
    "| yes  | 105| 87 | 192 | \n",
    "| no | 40| 62 | 102 |   \n",
    "| total | 145 | 149 | 294 | \n",
    " \n",
    "\n",
    "##### Table 3: Example of three-way contingency table\n",
    "| | |   Buy Exercise machine |     |     |\n",
    "|--- | :------------- | -------------:| :-----------:| :----------:| \n",
    "|**Customer group** | **Buy HDTV     ** | yes | no | total |\n",
    "|College students | yes  | 2| 9 | 11 | \n",
    "| | no | 5| 20 | 25 |\n",
    "| Working adults | yes  | 103| 78 | 181 | \n",
    "| | no | 35| 42 | 77 |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click to type your answer for 5.3.3 here:*\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
